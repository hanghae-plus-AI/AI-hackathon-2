,Unnamed: 0,title,link,text,summary
0,0,"GCP Vertex AI Pipelines 사용기
","https://surfit.io/link/Zp0zb
"," Sign up Sign in Junseong Park (Johan) Follow 당근 테크 블로그 131  당근 ML 인프라팀에서는 머신러닝 모델을 효과적으로 다양한 서비스에 적용하기 위해 노력하고 있어요. 이전 블로그 글(Kubeflow 파이프라인 운용하기)에서 언급했던 것처럼 당근마켓은 Google Kubernetes Engine(GKE)에서 Kubeflow Pipeline(KFP)을 설치해 적극 활용하고 있었는데요. 2023년 초기부터 Vertex AI Pipelines를 도입하고, 마이그레이션 해왔어요. 그리고 지난 10월에 드디어 전체 파이프라인을 Vertex AI Pipelines에 모두 옮기면서 KFP 및 이를 운영하기 위한 GKE 자원을 모두 제거했어요. 이번 글에서는 당근이 어떻게 TFX와 함께 Vertex AI Pipelines를 활용하고 있는지 공유해 보려 해요. Vertex AI Pipelines는 머신러닝 파이프라인을 Serverless 방식으로 관리 및 실행할 수 있는 Google Cloud Platform 제품이에요. ML 워크플로우에 필요한 데이터 전처리, 모델 훈련 / 평가, 배포 등등 머신러닝 프로젝트가 밟아야 하는 여러 단계를 Vertex AI 및 GCP 제품들로 구성하고, 자동화할 수 있어요. 2024년 1월 기준으로 Vertex AI Pipelines는 아래 두 가지 인터페이스를 지원하고 있어요. Kubeflow Pipelines(KFP) SDK TensorFlow Extended(TFX) SDK 기존에 잘 활용하고 있던 TFX SDK를 Vertex AI Pipelines와 함께 사용 중이에요. Managed Service인 Vertex AI 도입 이후 Kubernetes Cluster 운영 또한 필요하지 않아요. 그 때문에 운영보다 ML 워크플로우에 필요한 요소들을 개발하는 데 집중할 수 있어요. TFX는 프로덕션 머신러닝 파이프라인을 위한 end-to-end 플랫폼이에요. 2019년에 완전히 오픈소스화되었고, 컴포넌트 단위로 ML 워크플로우를 개발한 후 여러 환경(Apache Beam, Dataflow, Kubeflow, Airflow) 등에서 실행 가능해요. 물론 데이터 수집/변환, 학습, 배포 등을 위해 잘 작성된 컴포넌트도 같이 제공되고 있어요. TFX에 대한 좀 더 자세한 사항은 이전 블로그 글(TFX와 함께 머신러닝 파이프라인 개발하기)을 참고해 주세요. 머신러닝 모델이 실제 환경에서 잘 동작하려면 모니터링, 신뢰도와 유효성 검사 등 프로덕션 레벨의 요구사항을 잘 만족해야 해요. 이를 위해 TensorFlow Extended(TFX)를 적극 활용하고 있지만, TFX에서 제공하기 힘든 당근에서 필요로 하는 요구사항에 대해서는 ML 인프라팀이 나서야 해요. 당근은 각 서비스 조직이 정형화된 ML 인프라/MLOps 모듈들을 개발하는 데에 시간을 쏟지 않고, 담당하시는 서비스에 필요한 개발에 집중하길 바라요. 반대로 ML 인프라팀이 너무 한두 개의 서비스에 커플링되는 모듈에 집중한다면 계속 성장하며 늘어나는 ML 서비스 조직을 결국 감당하지 못할 거예요. 이런 고민들을 가지고 Vertex AI Pipelines를 활용하려 노력하고 있어요. TFX에서 기본으로 제공해 주는 컴포넌트를 통해서도 대부분의 ML 워크플로우를 소화할 수 있지만, 서비스 요구사항이나 생산성, 편의성을 향상하려다 보면 당근에 맞는 커스텀 컴포넌트가 필요한 순간이 오게 된다고 생각해요. tfx-addons 처럼 당근 내에서 자주 사용되는 TFX 컴포넌트들을 Python Package 형태로 관리하고, 배포해요. 각 팀에서 필요한 컴포넌트를 개발해 사용하면서도, 여러 부서에서 사용할 만한 컴포넌트들은 다 같이 공유하고 있어요. 이러한 방식을 통해 여러 서비스 조직에서 적은 공수로 생산성과 편의성을 취할 수 있고, 비용 및 성능 최적화가 필요한 시점에도 빠르게 전파가 가능해요. Vertex AI Pipelines는 KFP v1의 파이프라인들보다 아티팩트의 시각적인 표현이 강조되어 있어요. 기존 KFP v1에선 각기 컴포넌트가 어떤 아티팩트를 생성/소모되는지 표현되지 않았지만, 이제는 아래 그림처럼 컴포넌트 및 아티팩트 간 관계를 명확하게 보여줘요. 따라서 해당 아티팩트가 어떤 타입인지, 아티팩트가 어디 저장되어 있는지, 필요하다면 상호작용 버튼까지 제공하고 있어요. 위 StatisticsVisualizer로 통계상 이상한 데이터가 없는지 체크하기 위해 tfma 패키지를 활용해 html 파일을 생성하는 컴포넌트인데요. 해당 컴포넌트가 만들어내는 html 파일은 vertex_ai_stats_html 아티팩트에서 그 결괏값들을 저장하고, 필요하다면 사용자가 Web UI에서 그 결과를 바로 확인할 수 있게 만들어줘요. 이러한 기능들은 모델을 개발하시는 분들에게 여러모로 편의성을 제공해 주고 있어서 적극 활용하고 있어요. Recurring Run은 주기적으로 돌아야 하는 파이프라인에 CronJob처럼 스케줄을 걸어 반복해서 돌려주는 기능을 말해요. 해당 기능을 통해 매일 최신 유저 데이터를 받아 학습해야 하는 모델들을 일정 주기로 실행할 수 있는데요. Vertex AI Scheduler API가 등장하기 전까진 GCP의 Cloud Function과 Cloud Scheduler를 조합해서 Recurring Run을 구현하는 게 최선이었어요. 다만, 아직 TFX와 Vertex AI Scheduler API가 호환되지 않는 문제가 있어서 아래 구조도처럼 구성해 활용하고 있어요. 모델 학습 시 알맞은 CPU, Memory 및 GPU를 선택해 줘야 하는데요. KFP v2를 그대로 사용한다면, .set_cpu_limit()와 같은 함수를 통해 리소스 할당이 바로 가능하지만, TFX SDK와 Vertex AI Pipelines를 함께 사용한다면, DAG 컴파일 시 생성되는 파이프라인 스펙 JSON 파일을 직접 수정해 각기 자원들을 주입해 줘야 했어요. 이런 기능들을 위에서 언급한 확장 라이브러리에 추가해 별다른 수정 없이 전체 ML 엔지니어들이 활용할 수 있도록 제공하고 있어요. Vertex AI Pipelines는 Component 별로 리소스를 지정해 줄 수 있고, 이와 관련된 리소스 모니터링도 자체적으로 지원하고 있어요. 따라서 사용자들이 파이프라인에 문제가 발생했을 때 리소스와 관련된 문제인지 단번에 파악할 수 있고, 정상적으로 완료되는 상황에서도 리소스 효율을 체크해 학습 코드를 최적화하기 좋은 환경을 제공해 줄 수 있어요. 뿐만 아니라 위 정보들은 모두 Metric으로 남아 아래처럼 Dashboard를 만들거나, 서드파티 제품들과 데이터를 연동해 시각화하거나 알람을 만들 수 있어요. GCP에서 기본적으로 제공하는 Google Cloud Metric이 조직의 요구사항과 맞지 않다면, Custom Metric을 구성해 모니터링을 강화할 수 있어요. 예를 들어 Vertex AI Pipelines 로그 중 RUNNING, PENDING과 같이 지속되는 상태를 표현해야 하는 로그가 단발성으로 한 번만 로깅이 되어 현재 학습 중인 파이프라인의 개수나 팬딩중인 파이프라인들을 대시보드로 구성하기 어려워 직접 Custom Metric에 Continual State Event를 기록해 아래와 같이 대시보드로 구성해 현재 도는 파이프라인을 볼 수 있도록 모니터링 구성을 했어요. 이 밖에도 추가로 모니터링이 필요한 리소스나 데이터 통계들을 Google Cloud에 Custom Metric으로 등록해 기록한다면 Dashboard를 구성하거나 서드파티 제품들과 연동할 수 있어요. 위에서 언급한 Custom Metric을 통해 에러 및 특정 상황 발생 시 아래처럼 Slack 알람을 보내고 있어요. 담당자가 파이프라인에 문제가 생겼을 때 곧바로 인지할 수 있고, 관련 파이프라인 및 실험 페이지로 곧바로 이동할 수 있도록 구성했어요. 이를 통해 담당자 코드 에러인지 Vertex AI Pipelines 문제인지 에러 메시지들을 통해 쉽게 알 수 있어서, 온콜 부담 또한 많이 줄어들었어요. Vertex AI Pipelines가 Managed Service인 만큼 당근에서 활용하는 TFX 및 TensorFlow와 같은 ML 프레임워크 오픈소스들의 버전이 EoS(End of Support date)가 지났는지 체크하는 게 중요해요. 구글 측은 Vertex AI Docs — Supported frameworks for Vertex AI pipelines 문서에서 각 프레임워크의 EoS를 명시하고 있어요. EoS가 지나간 파이프라인 및 서비스들은 문제가 생겼을 때 Google Cloud Platform(GCP) 측에서 “버전 업그레이드”라는 대안을 제시할 가능성이 커요. 또한 해당 ML Framework들이 오픈소스이기 때문에 버전 호환성에 대한 책임이 GCP 측에 있지 않기에 미리 대응하지 않으면 잘 돌던 프로덕션 파이프라인들에 오류가 발생할 가능성이 있어요. 당근에서는 EoS를 체크하기 위해 아래처럼 대시보드 그래프를 구성하고, 모니터링하고 있어요. 각기 파이프라인의 EoS를 챙기는 과정에서 새로운 버전 기능이나 최적화 패치들이 적용되면서 더 좋아지는 경우도 생기고, 최신 버전들을 내용을 학습하며 다음 프로젝트를 구현할 때 참고하는 등 긍정적인 부분도 많아요. 당근 ML 인프라팀은 새로운 트렌드에 발 빠르게 대응하며 머신러닝 모델을 효율적이고 안정적으로 학습/배포할 수 있도록 노력 중이에요. 머신러닝이 서비스 활용도가 정말 높은 기술 중 하나라 믿고, 당근에서 머신러닝이 더욱 잘 활용될 수 있도록 노력하고 있어요. 전사 머신러닝 인프라/공통 컴포넌트 개발을 통해 당근 서비스에 지속해서 큰 임팩트를 만들고 싶은 동료를 찾고 있어요. Software Engineer, Machine Learning — ML 인프라 공고 Sign up to discover human stories that deepen your understanding of the world. Free Distraction-free reading. No ads. Organize your knowledge with lists and highlights. Tell your story. Find your audience. Membership Read member-only stories Support writers you read most Earn money for your writing Listen to audio narrations Read offline with the Medium app  131 Published in 당근 테크 블로그 당근은 동네 이웃 간의 연결을 도와 따뜻하고 활발한 교류가 있는 지역 사회를 꿈꾸고 있어요. Written by Junseong Park (Johan) Software Engineer, Daangn corp. More from Junseong Park (Johan) and 당근 테크 블로그 In 당근 테크 블로그 by Sunguck Lee VARCHAR vs TEXT 개요 In 당근 테크 블로그 by matthew l 🔥PyTorch Multi-GPU 학습 제대로 하기 PyTorch를 사용해서 Multi-GPU 학습을 하는 과정을 정리했습니다. 이 포스트는 다음과 같이 진행합니다. In 당근 테크 블로그 by mingrammer 추천 시스템의 심장, Feature Store 이야기 (1) 혼란 속의 질서 찾기: Feature Store를 구축하다 In 당근 테크 블로그 by Tommy Park 당근에서 LLM 활용하기 당근에서는 LLM을 어떻게 활용하고 있는지 소개해요 Recommended from Medium Christopher Adamson Implement End-to-End MLOps with SageMaker Projects Implementing robust machine learning pipelines remains a challenge for many organizations. Getting models from development into production… Dr Roushanak Rahmat Vertex AI Vertex AI is a comprehensive platform from Google Cloud that simplifies the development and deployment of artificial intelligence (AI)… Lists Predictive Modeling w/ Python Practical Guides to Machine Learning Natural Language Processing The New Chatbots: ChatGPT, Bard, and Beyond In Stackademic by Abdur Rahman Python is No More The King of Data Science 5 Reasons Why Python is Losing Its Crown In DataDrivenInvestor by Austin Starks I used OpenAI’s o1 model to develop a trading strategy. It is DESTROYING the market It literally took one try. I was shocked. In Towards Data Science by Eric Broda Agentic Mesh: The Future of Generative AI-Enabled Autonomous Agent Ecosystems Agentic Mesh is an ecosystem that lets Autonomous Agents find each other, collaborate, interact, and transact in a safe and trusted manner. In Towards Data Science by Cristian Leo How to Query a Knowledge Graph with LLMs Using gRAG Google, Microsoft, LinkedIn, and many more tech companies are using Graph RAG. Why? Let’s understand it by building one from scratch. Help Status About Careers Press Blog Privacy Terms Text to speech Teams","당근 ML 인프라팀에서는 머신러닝 모델을 효과적으로 다양한 서비스에 적용하기 위해 노력하고 있는데 당근이 어떻게 TFX와 함께 Vertex AI Pipelines를 활용하고 있는지 공유해 보려 한다. Vertex AI Pipelines는 TFX에서 기본으로 제공해 주는 컴포넌트를 통해서도 대부분의 ML 워크플로우를 소화할 수 있지만, 서비스 요구사항이나 생산성, 편의성을 향상하려다 보면 당근에 맞는 커스텀 컴포넌트가 필요한 순간이 오게 된다고 생각해 TFX에 대한 좀 더 자세한 사항은 이전 블로그 글(TFX와 함께 머신러닝 파이프라인 개발하기)을 참고해 주면 된다. Vertex AI Scheduler API가 등장하기 전까진 GCP의 Cloud Function과 Cloud Scheduler를 조합해서 Recurring Run을 구현하는 게 최선이었으나, 아직 TFX와 Vertex AI Scheduler API가 호환되지 않는 문제가 있어서 아래 구조도처럼 구성해 활용하고 있다. Vertex AI Pipelines는 Component 별로 리소스를 지정해 줄 수 있고, 이와 관련된 리소스 모니터링도 자체적으로 지원하고 있어 사용자들이 파이프라인에 문제가 발생했을 때 리소스와 관련된 문제인지 단번에 파악할 수 있고, 정상적으로 완료되는 상황에서도 리소스 효율을 체크해 학습 코드를 최적화하기 좋은 환경을 제공한다. ML 프레임워크 오픈소스들은 EoS가 지나간 파이프라인 및 서비스들이 문제가 생겼을 때 Google Cloud Platform(GCP) 측에서 “버전 업그레이드”라는 대안을 제시할 가능성이 커, ML 프레임워크 오픈소스들의 버전이 EoS가 지났는지 체크하는 게 중요하다. Published in 당근 테크 블로그 당근은 동네 이웃 간의 연결을 도와 따뜻하고 활발한 교류가 있는 지역 사회를 꿈꾸고 있다. uage Processing, Microsomous Data Science, Language Processing 등이다."
1,1,"왓챠 추천 서비스 MLOps 적용기 Part2
","https://surfit.io/link/Pl87x
","Sign up Sign in Realcyboys Follow WATCHA 98  안녕하세요. 왓챠 ML팀에서 머신러닝 엔지니어로 일하고 있는 찰스입니다. 이전 글에서는 기존 왓챠 ML 파이프라인 및 실험 환경이 가진 문제점에 대해서 살펴보고, 문제를 해결하기 위해 컨테이너 환경의 도입, On-premise GPU 서버와 클라우드 서비스와의 연동, ML 파이프라인과 실험 환경을 제공하기 위해 여러 서비스를 활용한 사례에 대해 살펴보았습니다. 이렇게 여러 해결책을 점진적으로 도입한 후 왓챠 ML 파이프라인 및 실험 환경은 기존에 비해 사용성과 안정성에서 많은 개선을 이루었습니다. 다만, 학습된 모델을 서빙하고 모니터링하는 과정은 여전히 개선해야 할 영역으로 남아있었습니다. 이 글에서는 왓챠 추천 시스템에서 학습된 추천 모델을 서비스에 반영하기 위해 독립적인 추론 서버가 왜 필요했고 어떻게 개발했는지, 그리고 추론 서버와 추천 모델의 성능을 모니터링하기 위해 어떠한 노력을 기울였는지에 대해서 알아보도록 하겠습니다. 기존 왓챠 추천 시스템은 추천 모델을 학습하고 추천에 필요한 여러 데이터를 정제하는 워커(Worker)와 워커에서 생성한 모델과 정제된 데이터를 이용하여 API 형태로 추천 결과를 제공하는 서비스(Service)로 이루어져 있었습니다. 이 중 워커는 이전 글에서 설명 드린대로 On-premise GPU 클러스터 내 Argo workflow를 기반으로 작성된 ML 파이프라인으로 이전되었고, ML 파이프라인에서 생성된 여러 모델 및 정제 데이터는 AWS S3와 Redis Pubsub을 통해 서비스로 동기화되어 실시간 반영되고 있었습니다. 서비스에서는 업데이트된 학습 모델과 정제 데이터를 활용하되, 하나의 애플리케이션에서 추천 후보 필터링, 전처리, 모델 추론, 후보정과 같은 과정을 거쳐 API 형태로 제공되었으며, 왓챠/왓챠피디아 내 추천 및 랭킹을 필요로 하는 클라이언트로부터 전달받은 여러 요청을 처리하고 있었습니다. 이러한 구조의 추천 서비스는 추천 모델과 추천에 필요한 데이터를 애플리케이션 내 메모리에 올려두어 사용하기 때문에 처리 속도가 매우 빠르다는 장점을 가지고 있었지만, Monolithic 서비스 특성상 특정 로직의 수정만으로도 서비스 전체에 배포되어야 하고, 모델의 문제가 추천 서비스 전체 장애로 퍼질 수 있다는 문제점을 가지고 있었습니다. 그리고 서비스는 동시성(Concurrency)과 비동기(Asynchronous)를 최대한으로 활용하기에 적합한 Scala로 구현되어 있어 PyTorch로 학습된 추천 모델을 추론하기 위해서는 PyTorch JNI와 같은 Java library가 필요했습니다. 아쉽게도 Pytorch JNI는 Pytorch에서 직접 관리하지 않아 버전 업데이트가 느렸으며 학습과 추론에 효율적인 기능이 PyTorch에 업그레이드되어도 JNI 버전이 업데이트되지 않아 실 서비스에서 빠르게 적용하기가 어려웠습니다. 또한, JNI를 이용하여 모델을 추론했을 때 기본적인 Forward 이외에 지원되는 기능이 제한되어 있어 추론에 필요한 여러 정보를 별도의 관리해야 하는 불편함이 있었습니다. 왓챠 ML 팀에서는 위에서 언급한 문제점을 해결하기 위해 독립적인 추론 서버를 구축하기로 했습니다. 독립적인 추론 서버를 구축하면 추가적인 서버 비용 및 운영 비용이 발생할 수 있지만, 앞서 언급한 문제점을 모두 해결할 수 있을 것이라 기대했습니다. 저희는 몇 가지 요구사항을 정해서 만족하는 추론 서버 프레임워크를 찾아보기로 했습니다. Pytorch로 학습된 모델(Pytorch eager model, TorchScript) 지원 여부 현재 왓챠/왓챠피디아 추천 시스템은 학습 프레임워크로 PyTorch만을 활용하므로, PyTorch에서 학습된 모델을 지원할 수 있는 서빙 프레임워크가 필요했습니다. Tensorflow와 같은 다른 학습 프레임워크에서 학습된 모델도 지원하면 좋으나 필수적인 요소는 아니었습니다. 2. HTTP 또는 gRPC 프로토콜 지원 여부 왓챠 서비스는 내부적으로 범용적인 통신 프로토콜인 HTTP나 gRPC을 이용해 통신하므로, 두 프로토콜 중 최소 하나의 프로토콜을 지원해야 했습니다. 3. CPU 추론 최적화 여부 왓챠 추천 서비스에 직접적으로 활용하는 모델들은 CPU 추론만으로도 충분한 성능을 낼 수 있는 경량화된 모델이므로 CPU 환경에서 최적화된 성능을 낼 수 있는 프레임워크가 필요했습니다. 추천을 제외한 일부 작업들은 GPU 추론이 필요했으나, 이는 실시간으로 트래픽을 받아서 처리해야 하는 작업이 아닌 배치 작업에 필요한 추론이었기 때문에 서빙 프레임워크의 GPU 추론 최적화가 필수 요구 조건은 아니었습니다. 4. 모니터링 용이 여부 독립적인 추론 서버는 모델의 추론 속도뿐 아니라 지연 시간(Latency), 처리량(Throughput), CPU 사용량, 메모리 사용량 등 주요 서버 지표도 실시간으로 모니터링할 수 있어야 합니다. 전사적으로 사용하고 있는 모니터링 도구인 데이터독(Datadog)과의 연동이 잘 되어야 했습니다. 5. Dynamic batching 지원 여부 Dynamic batching을 지원하는 서빙 프레임워크는 추론 서버에 들어온 요청 샘플들을 동적인 특정 미니 배치 단위로 묶어 모델에 추론을 요청하고 추론 결과를 받아볼 수 있는 기능을 제공했습니다. Dynamic batching을 활성화하면 지연시간은 약간 늘어날 수 있으나 처리량을 늘려 많은 수의 요청이 들어와도 안정적으로 추론 결과를 전달할 수 있어 지원 여부를 확인할 필요가 있었습니다. 위와 같은 요구 사항을 만족하는 추론 서버 프레임워크를 찾기 위해 TorchServe, Triton, Seldon Core, Fast API를 후보로 하여 각 프레임워크의 장단점을 비교했습니다. 우선 Fast API는 경량화된 웹 프레임워크이기 때문에, PyTorch로 학습된 모델을 추론하고 새로 학습한 모델을 교체하는 등의 모델 추론 서버의 기능을 거의 지원하지 않았습니다. 결과적으로 개발 자유도는 가장 높았지만, 저희가 추론 서버에 필요한 기능을 거의 모두 직접 구현해야 해서 고려 대상에서 가장 먼저 제외했습니다. NVIDIA Triton은 NVIDIA에서 만든 추론 서버 프레임워크로 NVIDIA GPU를 활용하여 고성능의 추론을 제공하고 추론 서버가 갖춰야 할 클러스터링, 노드 밸런싱, 오토 스케일링, Dynamic batching 등이 가능하여 대부분의 요구 사항을 만족했습니다. 다만, CPU 실행 환경을 직접 빌드 해야 하고 CPU 추론에 대한 지원이 미흡해서 후보에서 제외되었습니다. Seldon Core는 쿠버네티스 기반의 오픈소스 프레임워크로 대부분의 요구 사항을 만족했지만, Datadog integration을 직접적으로 지원하지 않고, 쿠버네티스에 특화된 프레임워크이기에 환경 설정에 대한 자유도가 적다는 점이 아쉬워 제외했습니다. 반면에 TorchServe는 모든 요구 조건을 부합하면서도 PyTorch 모델에 특화된 추가적인 최적화 기능들을 제공했습니다. 그리고 배포 및 추론에 특화되어있는 프레임워크답게 모델 배포, 관리, 스케일링, 모니터링과 같은 필수적인 기능을 모두 지원하여 추론 서버를 구축하는데 부족함이 없이 활용할 수 있다고 판단했습니다. 특히, IPEX(Intel extention for pytorch)를 통해 intel 기반의 CPU 환경에서 최적화가 잘 되어있었고, PyTorch ECO-system에 포함되어 지속적인 성능 향상 및 지원 확장에 대해 기대할 수 있다는 장점이 있었습니다. 위와 같은 이유로 저희는 TorchServe를 이용하여 추론 서버를 개발하기로 했습니다. 기존 모델 추론 방식은 추천 서비스 내 학습된 추천 모델을 직접 메모리에 올려 추론하는 방식이었기에 최소한의 모델 추론 속도 최적화만 필요했으나, 별도의 추론 서버를 구성하게 되면 추론 서버와의 네트워크 비용이 필연적으로 발생하기 때문에 전반적인 최적화가 필요했습니다. 우선, 별도의 서버를 구성하여 추론 모델 서비스를 제공하므로 지연시간 및 처리량 등과 같은 서버 주요 지표에 대한 성능 최적화가 필요했습니다. 또한, 다양한 경량화 기법으로 모델의 추론 속도를 빠르게 하면서도 추천 모델의 정확도는 최대한 유지할 수 있어야 했습니다. 이 두 가지 측면에서 최적화를 이룰 수 있도록 어떠한 노력들을 했는지 살펴보도록 하겠습니다. a. TorchServe 옵션 최적화 TorchServe는 학습된 모델을 빠르고 안정적으로 추론하기 위해 여러 옵션을 제공합니다. 이 중 저희가 테스트를 했을 때 성능 개선에 유효했던 옵션들에 대해서 살펴보도록 하겠습니다. default_workers_per_model 이 옵션은 각 모델마다 기본 몇 개의 워커를 할당할지 결정하는 옵션입니다. 저희는 추천 모델마다 별도의 TorchServe를 구성하지 않고 여러 추천 모델을 동시에 올려서 사용하는 것을 전제로 했기에 이 옵션이 비교적 중요했습니다. 이 값은 현재 가용한 CPU 개수와 밀접한 관련이 있는데, CPU 개수보다 너무 많은 워커 프로세스가 생성되면 CPU에 할당되는 워커 프로세스가 변경될 때마다 불필요한 오버헤드가 발생할 수 있습니다. 또한, 너무 적은 워커 프로세스를 생성하면, 유후(Idle) CPU 코어가 생기므로 적절한 숫자를 고려해야 합니다. 또한, 워커 프로세스가 증가할 때마다 모델을 메모리에 올려야 하므로 메모리 사용량을 고려하여 조정해야 합니다. 저희는 여러 실험을 해본 결과, default_workers_per_model는 활용 가능한 physical core 개수와 동일할 때 가장 좋은 효율을 보였습니다. (참고로 pytorch의 thread count는 1로 고정하여 테스트를 진행하였습니다.) 2. ipex_enabled IPEX(intel pytorch for extension)는 pytorch로 학습된 모델이 intel CPU 위에서 동작할 때 최적화해주는 도구입니다. 해당 옵션을 활성화하면 별다른 처리를 하지 않아도 최적화 과정을 거쳐 추론을 하게 됩니다. 특별한 경우가 아니라면 활성화하는 것이 성능 향상에 도움이 됩니다. 3. cpu_launcher_enable, use_logical_core 기본적으로 CPU logical thread를 위에서 GEMM(General matrix multiply) 연산을 수행할 때 병목이 발생할 수 있기 때문에 physical core에 고정(pinning) 하는 것을 권장합니다. cpu_launcher_enable 옵션은 physical core 혹은 logical core에만 프로세스를 할당하고, use_logical_core 옵션을 통해 제어할 수 있습니다. 저희는 physical core에만 고정할 수 있도록 옵션을 지정했고, physical core에 고정했을 때 성능 차이에 대한 자세한 내용은 링크에서 확인하실 수 있습니다. 4. batch_size, batch_delay Dynamic batching을 활용하기 위해서는 모델에서 한 번에 처리할 미니 배치의 최대 사이즈인 batch_size와 dynamic batching을 수행할 최소 대기 시간인 batch_delay를 지정해야 합니다. 이는 모델의 특성과 입력 값에 따라 최적화된 값이 다를 수 있으므로 실험을 통해 최적화된 값을 지정하는 것이 좋습니다. 이 외에도 NUMA(Non uniform memory access) 활용, OpenMP 활용, Memory allocator 변경 등의 CPU 관련 최적화 작업은 링크1, 링크2에서 참고하시기 바랍니다. b. 네트워크 비용 최소화 기존 왓챠/왓챠피디아 추천 서비스에서는 추론 모델을 Torchscript로 빌드 하여 애플리케이션 내에서 직접 추론했습니다. 그렇기 때문에 별도의 통신 비용이 발생하지 않았으나 별도의 추론 서버를 분리한 이후에는 필연적으로 네트워크 비용이 발생할 수밖에 없었습니다. 특히, 사용자의 감상, 평가 같은 이력 데이터(Historical data)는 계속 데이터가 늘어나게 되어 이를 입력 값으로 사용하는 모델은 많은 네트워크 비용을 발생시킬 가능성이 높아지게 됩니다. 저희는 이런 문제를 해결하기 위해 아래 방법을 활용했습니다. 캐싱(Caching) 추천 모델에서는 사용자의 다양한 행동 이력을 활용하는 경우가 많습니다. 일반적인 경우에는 문제가 되지 않지만, 많은 이력 데이터를 가지고 있는 일부 헤비 유저들의 추천을 위해 반복적으로 이력 데이터를 모두 전달하는 것은 비효율적입니다. 저희는 일부 헤비 유저들의 이력 데이터를 메모리 캐시에 저장하고 최근 발생한 이력 데이터만 추론 서버에 요청하면, 캐싱 된 이력 데이터와 병합한 후 추론하는 방식으로 개선할 수 있었습니다. 2. 바이트 단위의 직렬화 및 역 직렬화 일반적으로 네트워크 통신 효율화를 위해 JSON, XML과 같은 최소한의 가독성을 보유하는 데이터 포맷을 사용하는 대신 프로토콜 버퍼(Protocol buffer)나 메시지 팩(Message pack)과 같은 바이트(Byte) 단위로 직렬화, 역 직렬화하여 전송되는 데이터의 양을 최소화하는 방법을 사용합니다. 저희는 내부적으로 프로토콜 버퍼와 메시지 팩을 모두 활용하고 있었지만, 추론 서버와의 통신에서는 비교적 적은 양의 데이터를 직렬화할 때 조금 더 속도가 빠른 메시지 팩을 활용하여 추론 속도를 개선했습니다. c. 모델 배포 안정화 TorchServe는 모델 추론 서빙 프레임워크로 새로 학습된 모델을 배포하여 교체하기 위한 여러 management API를 제공합니다. 하지만 management API를 이용해서 새로운 모델을 배포하면 일정 시간 동안 추론 시간이 불안정하게 증가하는 문제가 발생했습니다. 저희는 이 문제를 해결하기 위해, 등록된 모델을 기본 모델로 변경하기 전에 일정 시간 warm-up을 진행했고, 초기 추론 속도가 느린 문제를 어느 정도 완화할 수 있습니다. 그럼에도 불구하고 문제를 완전히 해결할 수 없었고, 아래와 같이 프로파일링 옵션을 비활성화하여 모델을 교체한 직후에도 지연 시간이 증가하지 않고 안정적으로 모델을 배포할 수 있게 되었습니다. 별도의 추론 서버를 분리하면 네트워크 통신으로 인해 추가적인 지연 시간이 발생하기 때문에 모델 자체의 추론 시간을 최소화할 필요가 있었습니다. 저희는 아래의 3가지 경량화 기법을 활용하여 최소화를 진행하였습니다. 가지치기(Pruning) 불필요한 매개변수 또는 가중치를 제거하여 모델을 최적화하는 방법입니다. 학습된 모델에서 가중치가 작은 연결 또는 중요하지 않은 연결을 제거하여 더 효율적으로 메모리를 사용하고 실행 속도를 높일 수 있습니다. 2. 양자화(Quantization) 실수 범위의 값을 정수 범위의 값으로 변환하는 방법입니다. 일반적으로 실수 범위의 값은 FP32(32 비트), 정수 범위의 값은 INT8(8 비트)을 사용하는데, 줄어드는 비트만큼 전체 메모리 사용량 및 수행 속도가 감소하는 효과를 얻을 수 있습니다. 또한, CPU에서 정수형 연산이 최적화되어있기 때문에 조금 더 성능 향상을 꾀할 수 있습니다. 3. 지식 증류(Knowledge distillation) 지식 증류(Knowledge distillation)는 커다란 교사 모델(Teacher model)을 조그마한 학생 모델(Student model)로 학습한 지식을 전달하는 방법입니다. 이 기술은 주로 교사 모델의 지식을 학생 모델로 전달하여 더 경량화된 모델을 만들거나, 정확도를 높이는 데 사용됩니다. 아쉽게도 모든 방법이 성능 향상에 도움을 주지는 못했고, 추천 모델마다 모델 아키택쳐 특성이 달라 특정 모델에서 성능 향상을 보였던 방법이 다른 방법에서는 효과를 보지 못하거나 반영할 수 없기도 했습니다. 또한 추론 속도를 향상시킬 수는 있었으나 모델 정확도가 떨어질 수 있어 하이퍼 파라미터 조정을 같이 염두하며 경량화를 진행했습니다. 결과적으로는 모델마다 상이하지만 최대로 경량화된 모델을 기준으로 오프라인 정확도 감소를 1% 미만으로 최소화하면서도 모델 추론 속도를 50%가량 향상시킬 수 있었습니다. 추천 서비스에 모니터링 시스템이 필요하듯이 독립된 추론 서버를 구성하면 이를 위한 별도의 모니터링 시스템이 필요합니다. 저희는 전사적으로 Datadog을 사용하여 여러 지표를 확인하고 있기 때문에 Datadog과 바로 연동할 수 있는 TorchServe를 서빙 프레임워크로 선택하기도 했습니다. TorchServe는 metrics_mode 옵션을 통해 프로메테우스(Prometheus) 형태의 메트릭 로그를 쌓을 수 있고, Datadog에서는 서비스 내 로그 파일 경로만 지정하면 간단하게 여러 지표를 그래프로 표현해주었습니다. 이 외에도 위 그림처럼 쿠버네티스 관련 메트릭이나, 각 모델의 추론 속도, 서비스 로그 등을 대시보드화 하여 만들 수 있어 서버 관련 지표를 한눈에 살펴볼 수 있습니다. 또한, 서비스 자체의 서비스 수준 목표(Service Level Objectives)를 지정하여 이를 벗어나는 지표를 보이는 경우 알림을 주어 문제 상황에 빠르게 대응할 수 있도록 했습니다. 또한, 학습된 추천 모델을 이용하여 실 서비스에 추천한 결과가 얼마나 효과적이었는지 판단할 수 있게 온라인 지표를 볼 수 있는 방법도 필요합니다. 저희는 기존부터 Google Looker studio에 실 서비스에서 쌓인 로그 데이터를 가공하여 노출 대비 클릭, 평가, 감상 등과 같은 주요 지표를 확인했으며, 국가, 구독/비구독 여부, 클라이언트 타입 등에 따라 각 지표를 필터링하여 지속적인 성능 모니터링을 해왔습니다. 결과적으로 두 가지 대시보드를 통해 추론 서비스의 서버 지표와 추천 모델의 온라인 지표를 확인할 수 있게 되었고, 학습된 모델이 실 서비스에서 잘 활용되고 있는지 판단할 수 있는 근거로 활용할 수 있게 되었습니다. 현재 왓챠/왓챠피디아 추천 시스템의 구조는 추론 서버를 분리한 후 위 그림과 같은 구조로 변경되었습니다. 우선, 추론 서버를 분리하여 모델 추론에 대한 작업만 관리할 수 있는 별도의 서비스를 구축하게 되어 모델의 문제가 API 서비스로 전이되지 않게 되었습니다. 또한, JNI의 의존성이 제거되어 독립적으로 최신 PyTorch 버전 업데이트를 할 수 있게 되었고 필요한 정보를 모델에 직접 주입하여 JNI로 인한 제약에서 벗어날 수 있게 되었습니다. 이러한 개선 이외에도, 분리된 추론 서버에 범용적인 인터페이스를 제공하여 추천 서비스 이외에 추론 결과를 필요로 하는 모든 서비스에서 활용할 수 있는 구조를 만들 수 있었습니다. 일반적으로 학습된 모델의 입력 값과 출력값은 모델에 맞게 변환된 값이므로 실제 데이터를 바로 모델 추론에 활용할 수 있도록 변환하는 과정이 필요합니다. 예를 들어, 콘텐츠마다 임의의 ID를 부여하거나, 텍스트를 토크나이저(Tokenizer)를 통해 임의의 양수 값의 순서로 변환하는 과정이 필요할 수 있습니다. 이러한 전후 처리는 학습된 모델에 의존성을 가지고 있기 때문에 모델 별로 관리되어야 했습니다. 다만, 학습된 추천 모델은 왓챠/왓챠피디아 서비스에서 활용할 뿐 아니라 다른 서비스에서 모델 추론 결과를 활용하는 경우(예를 들어, 사용자별 다음 시청 확률을 개인화 추천 푸시에 활용하거나 예상 별점을 콘텐츠 수급에 활용)가 있었습니다. 이를 위해 각 서비스에서 중복되는 전후 처리를 구현해야 하는 번거로움이 있었습니다. 결과적으로 위 그림처럼 추론 서버 내 핸들러를 통해 전후 처리를 진행하게 되어 추천 서비스뿐 아니라 추론 결과를 원하는 다른 서비스에서도 전후 처리에 대한 고려 없이 쉽게 추론 결과를 받아볼 수 있는 구조를 마련하게 되었습니다. 지금까지 왓챠 ML팀이 왓챠/왓챠피디아 추천 시스템에 MLOps를 어떻게 적용했는지 살펴보았습니다. 결론적으로 왓챠 ML 팀은 별도의 GPU 서버에서 쿠버네티스와 Argo workflow를 활용하여 ML 파이프라인을 구성하고, TorchServe로 독립적인 추론 서버를 구축하여 학습된 모델을 다양한 서비스에 활용할 수 있는 End-to-end 파이프라인을 만들 수 있게 되었습니다. 마지막으로 그동안 왓챠 추천 서비스에 MLOps를 적용하기 위해 같이 고생해 주신 폴, 매튜, 루이스, 제이크, 로기, 그리고 블로그 글 발행에 도움 주신 대런, 제인에게도 감사의 말을 전합니다. 감사합니다. Sign up to discover human stories that deepen your understanding of the world. Free Distraction-free reading. No ads. Organize your knowledge with lists and highlights. Tell your story. Find your audience. Membership Read member-only stories Support writers you read most Earn money for your writing Listen to audio narrations Read offline with the Medium app  98 Written by Realcyboys WATCHA More from Realcyboys and WATCHA Realcyboys in WATCHA 왓챠 추천 서비스 MLOps 적용기 Part1 안녕하세요. 왓챠 ML팀에서 머신러닝 엔지니어로 일하고 있는 찰스입니다. Jisu Jeong in WATCHA GNN 소개 — 기초부터 논문까지 이 글은 Shanon Hong의 An Introduction to Graph Neural Network(GNN) For Analysing Structured Data를 저자에게 허락받고 번역, 각색한 글이다. Jisu Jeong in WATCHA 그래프 임베딩 요약 이 글은 Primož Godec의 Graph Embeddings — The Summary를 허락받고 번역한 글입니다. marong61 in WATCHA Wasm 간단 사용기 2015년에 WebAssembly(이하 wasm)가 탄생하고 2017년에 주요 브라우저에서 wasm 을 사용할 수 있게 된 이후로 다양한 방법으로 wasm 가 사용되고 있습니다. Recommended from Medium Abdur Rahman in Stackademic Python is No More The King of Data Science 5 Reasons Why Python is Losing Its Crown Austin Starks in DataDrivenInvestor I used OpenAI’s o1 model to develop a trading strategy. It is DESTROYING the market It literally took one try. I was shocked. Lists Predictive Modeling w/ Python Practical Guides to Machine Learning Natural Language Processing The New Chatbots: ChatGPT, Bard, and Beyond Harendra How I Am Using a Lifetime 100% Free Server Get a server with 24 GB RAM + 4 CPU + 200 GB Storage + Always Free AI Rabbit in CodeX Has Anthropic Claude just wiped out an entire industry? If you have been following the news, you may have read about a new feature (or should I call it a product) in the Claude API — it is… Wasim Rajput in The Generator Developing Large Language Models (LLMs): A Step-by-Step Guide from Concept to Deployment How LLMs like ChatGPT, Gemini, and Others are Developed Don Lim GPT-5 is finally here — What To Expect and What Not To Expect Microsoft engineers are preparing to host Orion (GPT-5) on Azure as early as November 2024. Help Status About Careers Press Blog Privacy Terms Text to speech Teams","왓챠 ML팀에서 머신러닝 엔지니어로 일하고 있는 찰스는 왓챠 추천 시스템에서 학습된 추천 모델을 서비스에 반영하기 위해 컨테이너 환경의 도입, On-premise GPU 서버와 클라우드 서비스와의 연동, ML 파이프라인과 실험 환경을 제공하기 위해 여러 서비스를 활용한 사례에 대해 살펴보았다. Pyorch로 학습된 추천 모델을 추론하기 위해서는 Pytorch JNI와 같은 Java library가 필요했는데 Pytorch에서 직접 관리하지 않아 버전 업데이트가 느리고 JNI 버전이 업데이트되지 않아 실 서비스에서 빠르게 적용하기가 어려웠다. TorchServe, Triton, Seldon Core, Fast API를 후보로 하여 각 프레임워크의 장단점을 비교한 결과, Fast API는 경량화된 웹 프레임워크이기 때문에, PyTorch로 학습된 모델을 추론하고 새로 학습한 모델을 교체하는 등의 모델 추론 서버의 기능을 거의 지원하지 않았으나 저희가 추론 서버에 필요한 기능을 거의 모두 직접 구현해야 겠다. NVIDIA Triton은 NVIDIA에서 만든 추론 서버 프레임워크로 고성능의 추론을 제공하고 추론 서버가 갖춰야 할 클러스터링, 노드 밸런싱, 오토 스케일링, Dynamic batching 등이 가능하여 대부분의 요구 사항을 만족했지만, CPU 실행 환경을 직접 빌드 해야 하고 CPU 추론에 대한 지원이 미흡해서 고려 대상에서 가장 먼저 제외되었다. TorchServe는 학습된 모델을 빠르고 안정적으로 추론하기 위해 여러 옵션을 제공하는데, 이 중 성능 개선에 유효했던 옵션들에 대해서 살펴보도록 하겠다. CPU logical thread를 위에서 GEMM 연산을 수행할 때 병목이 발생할 수 있기 때문에 physical core에 고정(pinning)하는 것을 권장하며 사용자의 감상, 평가 같은 이력 데이터(Historical data)는 데이터가 늘어나게 되어 많은 네트워크 비용을 발생시킬 가능성이 높아지게 된다. 많은 이력 데이터를 가지고 있는 헤비 유저들의 추천을 위해 반복적으로 이력 데이터를 전달하는 것은 비효율적이기 때문에 저희는 일부 헤비 유저들의 이력 데이터를 메모리 캐시에 저장하고 최근 발생한 이력 데이터만 추론 서버에 요청하면, 캐싱 된 이력 데이터와 병합한 후 추론하는 방식으로 개선할 수 있었다. 불필요한 매개변수 또는 가중치를 제거하여 모델을 최적화하는 방법인 가지 경량화 기법을 활용하여 불필요한 매개변수 또는 가중치를 제거하여 모델을 최적화하는 방법이다. TorchServe는 metrics_mode 옵션을 통해 프로메테우스(Prometheus) 형태의 메트릭 로그를 쌓을 수 있고, Datadog에서는 서비스 내 로그 파일 경로만 지정하면 간단하게 여러 지표를 그래프로 표현해주어 학습된 모델이 실 서비스에서 잘 활용되고 있는지 판단할 수 있는 근거로 활용할 수 있게 되었다. 왓챠 ML 팀은 별도의 GPU 서버에서 쿠버네티스와 Argo workflow를 활용하여 독립적인 추론 서버를 구축하여 학습된 모델을 다양한 서비스에 활용할 수 있는 End-to-end 파이프라인을 만들 수 있게 되었다."
2,2,"돈이 되는 Data Analytics
","https://surfit.io/link/nERpp
","Posting Recruiting Contact 최정현
2024-03-04 안녕하세요, Tech Data AI검색 소속 데이터 분석가 최정현입니다. 지난해 말부터 커머스 유저 데이터를 분석 및 지표를 기획하고 대시보드로 자동화 하는 업무를 진행하였습니다. 이를 통해 추가적인 수익을 얻게 되었고 데이터를 통한 최적화 방향을 잡을 수 있었습니다. Databricks와 AWS 서비스를 활용한 데이터 파이프라인 설계 및 고도화 부분은 해당 작업을 맡아주셨던 같은 팀 권순철님이 작성해주셨습니다. 먼저 대시보드를 만들게 된 계기와 구체적인 지표 선정 작업을 공유드려요. 그리고 Databricks와 AWS 서비스를 활용한 데이터 파이프라인 설계 과정과 대시보드의 임팩트 그리고 개선할 점으로 글을 구성했습니다. 🏐 들어가며 지난 11월, 브랜디 서비스는 메인페이지 추천 및 비즈니스 로직에 대한 실험을 진행했습니다. 그 결과, 실험을 진행한 조닝*에서 실험 집단이 통제 집단에 비해 압도적인 거래량을 발생시켰지만 실험 대상이 아닌 타 조닝에서 통제 집단이 실험 집단에 비해 높은 거래액을 발생시켰어요. 결국 상쇄된 거래액으로 인해 기존의 상품 진열 비즈니스 로직을 제거하지 않는 방향으로 결론이 났습니다. *조닝: 상품이 전시되는 영역 해당 실험의 결과를 좌우한 핵심 지표는 ‘거래액’이었지만, 저희 팀은 이커머스 맥락에 맞춰 AARRR 지표를 위주로 추적하며 유저의 서비스 이용 과정에서의 어떤 차이가 해당 조닝에 대한 거래액 차이를 만들어냈는지 원인을 파악하는 작업을 진행했습니다. 여기서 AARRR이란, 사용자의 서비스 이용 여정을 다섯 단계로 나누어 분석하는 프레임워크입니다. Activation(고객 획득), Activation(활성화), Retention(재방문율), Revenue(수익률), Referral(추천)으로 구성되어 있습니다. 아래로 갈수록 사용자의 비율이 줄어드는 깔대기 모양으로, 더 많은 사용자를 각 단계에서 그 다음 단계로 전환시키는 것을 목적으로 합니다. 궁극적으로는 사용자가 해당 서비스를 더 적극적으로 이용하고 더 많은 수익을 창출하는 건강한 사용자 여정을 만들고자 하는 것이죠. 해당 실험에서는 이 중에서도 Activation, Retention, Revenue 관련 지표를 개발하여 실험 결과를 측정했습니다. 실험 기간인 한 달 간, 아래와 같이 매일 아파치 스파크를 활용하여 지표를 수동으로 추출 및 실험 리포트를 작성했는데요. 이 과정에서 실험 이후에도 해당 지표를 꾸준히 추적할 필요를 느꼈습니다. 따라서 해당 작업을 자동화하고 더욱 세밀한 AARRR 분석을 위해 대시보드 작업을 시작하였습니다.  🥎 기존 고객 이탈을 방지하기 위한 AR 대시보드 이전에는 대시보드가 없었나요? 기존 태블로 대시보드가 존재했습니다만, (1) 다양한 차트와 대시보드가 산발적으로 존재해 서비스의 현황을 한눈에 진단하기 어려웠습니다. (2) 사업부의 요구사항에 따라 그때 그때 필요한 내용을 작성한 경우가 많아, 데이터 분석가가 서비스를 바라보는 관점은 포함되지 않았습니다. (3) 대부분 거래액과 같은 ‘결과 지표’ 위주로 집계하는 방식으로 작성되어 있었으며 ‘선행 지표’인 플랫폼 내부의 유저의 여정에 대한 지표는 따로 확인하고 있지 않았습니다. 요컨대, 데이터 분석가의 관점을 반영해 필요한 지표를 통합하여 서비스의 현재 상태를 통합적으로 볼 필요가 있었습니다. 단순 결과 지표 뿐 아니라 이에 선행하는 ‘선행 지표’를 추적함으로써 서비스 유저 여정의 단계별 현황을 파악하고 유저가 겪는 문제를 진단하는 것이 시급했지요. 새로운 대시보드는 무엇을 보여주나요? 좋은 대시보드는 데이터 분석가가 서비스를 바라보는 관점, 대시보드 독자에게 전하고자 하는 주장이 스토리라인으로 명확히 담겨있어야 합니다. 새로이 기획할 대시보드에 이를 반영해야 했지요. 이전 실험 결과에 더불어 거래액 등 주요 결과지표의 흐름을 봤을 때, “기존 고객이 지속적으로 높은 비율로 이탈하는 것”이 서비스가 당면한 문제라는 결론을 내릴 수 있었기에 대시보드의 관점을 다음과 같이 설정하였습니다.  왜 Activation과 Retention이죠? AARRR 중에서도 가장 중요한 것은 Activation과 Retention입니다. 아무리 고객을 새로 유입시켜도 고객의 활동성이 높아지고 리텐션이 낮아지면 결국 밑빠진 독처럼 구매 전환으로 가는 고객의 비율이 높아지기 어렵기 때문입니다. 따라서 고객의 이탈을 막기 위해 독의 밑바닥을 채우기 위해서는 Activation과 Retention 단계에서의 지표 개선이 이루어져야 합니다. 뿐만 아니라 이미 Revenue로 대표되는 ‘결과 지표’들은 충분히 다른 팀에서도 많이 보고 있었기에 아직 사내에서 제대로 확인하지 않던 Activation과 Retention을 위주로 지표를 선정했습니다. 구체적으로 어떤 지표를 작성했나요? 고객이 가장 많이 방문하는 메인페이지, 검색 지면, 상품 상세 페이지, 총 3가지 지면을 위주로 Activation과 Retention 관련 지표를 선정하고 개발하였습니다. Activation Activation의 경우 고객의 활동성을 정의할 수 있는 방식이 다양하기 때문에 각 지면별로 활동성에 관한 주요 질문을 던지고 질문의 답이 될 수 있는 지표를 제시하는 방식으로 개발하였습니다. 핵심 질문과 그에 대한 답변이 되어주는 지표를 나열하면 아래와 같습니다. (1) 메인페이지 Q1. 고객은 브랜디의 홈 탭에서 얼마나 스크롤을 내리는가? 메인페이지의 영역별 노출 유저 수 비율: 메인페이지에서 각 영역(메인배너, 최상단 조닝, 무한추천영역)별로 몇 명의 고객이 노출됐는가? 특정 조닝 마지막 노출 상품의 진열 순서 Q2. 각 조닝의 상품 노출 상황 및 효과는 어떠한가? 영역별 상품 CTR, 노출된 상품 수 , 셀러 수 Q3. 고객이 메인페이지에서 본 상품에 대해 다른 페이지보다 얼마나 더 특정 행동을 수행하는가? 메인페이지의 찜, 상세페이지 클릭 수 등 홈에서 클릭한 상품에 대한 구매까지의 퍼널 (2) 상세 페이지 Q1. 상세 페이지는 충분히 다양하게 노출되고 있는가? 전체 상품 대비 4번 이상 조회된 상품 수 등 Q2. 상세페이지 내 각 조닝의 상품 노출 상황 및 효과는 어떠한가? 조닝별 CTR (3) 검색페이지 Q1. 고객이 검색을 통해 본 상품에 대해 다른 페이지보다 얼마나 더 특정 행동을 수행하는가? 검색 상품 클릭수 검색에서 진입한 상품에 대한 구매까지의 퍼널 등 Q2. 검색 결과 지면에서 광고상품과 비광고 상품의 효율은 어떻게 다른가? 광고 여부별 상품 CTR 및 클릭 수 Q3. 검색어랭킹과 변화는 어떠한가? 급상승 검색어, 최근 한달 검색어 랭킹 Q4. 검색 이전과 이후 행동은 어떻게 되는가? 검색 이전 행동 event flow, 검색 이후 행동 event flow Activation 단계에서는 퍼널 분석을 적극 활용했습니다. 예를 들어, 홈에서 상품 상세페이지를 클릭한 경우 대비 검색에서 상품 상세 페이지를 클릭한 경우 찜과 장바구니, 구매까지 가는 비율을 비교하였습니다. 2. Retention Retention 지표의 경우 코호트 분석을 결합하여 지표가 떨어지는 원인을 찾고자 시도했습니다. 전체 방문 리텐션 및 구매 리텐션을 구하고 구매자와 비구매자로 코호트를 나누어 방문 리텐션을 확인했습니다.  🏀 데이터 파이프라인은 어떻게 설계하고 무엇을 발견했나요? 현재 저희팀은 검색, 추천 그리고 CRM 최적화 작업을 진행하고 있습니다. 그리고 앞에서 정현님이 언급한 Ad-hoc한 분석을 진행하면서, 비즈니스와 밀접한 광고실 및 유관 부서에 데이터 분석 리포팅을 제공하고 있어요. 데이터 파이프라인을 만들고 운영하면서 데이터 분석가를 포함한 데이터 관련 팀원 분들, 비즈니스 문제와 직접적으로 맞닿은 유관 부서분들 그리고 서비스를 이용하는 고객 모두를 고려해야 한다는 점을 알 수 있었습니다. 결국 데이터 파이프라인을 설계할 때도 도메인에 대한 이해가 필요합니다. 🔥 패션 커머스 플랫폼의 특이점 패션 커머스는 어떤 특징을 가지고 있을까요? 앞에서 팀이 서비스의 전반적인 상황을 종합적으로 파악하기 위한 동기 부분에서 말씀드린 대로 패션 커머스 플랫폼은 크게 세 축으로 구성되어 있습니다. 세 축은 각각 {상품, 고객, 셀러} 입니다. 고객과 셀러는 상품으로 연결됩니다. 플랫폼 특성상 상품, 고객, 셀러 모두를 고려해야 합니다. 이 중에서 한 축을 놓치는 순간 플랫폼은 급격히 무너집니다. 다음으로 이커머스에서 고객의 행동 경로를 살펴보면 아래와 같은데요. 고객이 상품과 밀접하게 행동하는 주요 페이지는 메인 페이지, 상품 상세 페이지 그리고 검색 페이지인 것을 데이터 분석을 통해 확인할 수 있습니다. 다시 말해서 위의 세 축(상품, 고객, 셀러)과 함께 페이지간의 관계를 파악하기 위한 데이터 분석이 필요했습니다. 그리고 데이터 분석을 원활하게 하기 위해서 미리 각 축을 기준으로 정리된 데이터가 필요함을 알 수 있습니다. ⚽ 비즈니스 문제를 풀기 위한 데이터 파이프라인 데이터 파이프라인은 결국 조직이 마주친 비즈니스 문제를 풀고 현상을 파악하기 위해서 존재합니다. 비즈니스를 풀기 위해서 Analytics 파이프라인과 ML 파이프라인이 존재합니다. 결국 데이터 엔지니어링 라이프 사이클을 추상화하면 아래 이미지와 같은데요. 앞에서 말씀드린 여러 태스크와 리포팅이 공통적으로 사용하는 데이터는 유입 데이터, 고객의 행동 데이터 그리고 커머스 서비스를 위한 트랜잭션 데이터입니다. 데이터 파이프라인을 고안 및 개선하면서 느낀 점들은 크게 두 가지입니다. 먼저 배치 및 스트리밍 데이터 파이프라인을 구성할 때, 파이프라인 자체에서 발생할 수 있는 데이터 정합성 오류와 병목현상을 최소화하는 것입니다. 두번째는 데이터 파이프라인이 생산하는 최종 결과물의 대상 고객과 고객이 원하는 것을 확실히 파악하는 것입니다. 이 때 중요한 것은 도메인에 대한 이해입니다. 1. 데이터 파이프라인 자체에서 발생할 수 있는 내재적인 문제 다루기 팀에서 활용하는 데이터 파이프라인의 기본 저장소는 S3입니다. 여러 형태의 데이터를 S3에 쌓는 구조를 효율적으로 처리하기 위해서 저희는 Medaillion architecture를 차용하였습니다. Medallion architecture의 구조는 아래와 같습니다. 위의 아키텍처를 이커머스 도메인과 팀의 태스크에 맞게 적용하고 보완하면 다음과 같은 데이터 파이프라인을 고안할 수 있습니다. Bronze data layer
Bronze 레이어에는 내부 RDS 내 데이터와 외부 소스 시스템에서 로깅된 모든 데이터가 저장됩니다.
외부 소스에는 현재 앱스플라이어 데이터 라커 데이터와 앰플리튜드 데이터 그리고 Impression 데이터가 존재합니다.
데이터 적재시 신선도(Freshness) 측면에서 가장 다운 타임이 증가하는 데이터 레이어입니다. Bronze 레이어에는 내부 RDS 내 데이터와 외부 소스 시스템에서 로깅된 모든 데이터가 저장됩니다. 외부 소스에는 현재 앱스플라이어 데이터 라커 데이터와 앰플리튜드 데이터 그리고 Impression 데이터가 존재합니다. 데이터 적재시 신선도(Freshness) 측면에서 가장 다운 타임이 증가하는 데이터 레이어입니다. Silver data layer
Bronze 레이어 내 데이터를 주요 도메인의 축 기준으로 ETL한 뒤 적재합니다.
Silver 레이어에는 Gold 레이어에 도달하기 전 지표 추출, 데이터 분석, ML을 지원하기 위해 존재합니다.
데이터 분석가, 데이터 사이언티스트, ML 엔지니어가 주로 사용하는 데이터 레이어입니다. Bronze 레이어 내 데이터를 주요 도메인의 축 기준으로 ETL한 뒤 적재합니다. Silver 레이어에는 Gold 레이어에 도달하기 전 지표 추출, 데이터 분석, ML을 지원하기 위해 존재합니다. 데이터 분석가, 데이터 사이언티스트, ML 엔지니어가 주로 사용하는 데이터 레이어입니다. Gold data layer
현재 Gold 레이어도 S3에 저장하지만 프로젝트별로 다른 경로로 저장됩니다.
태블로와 같이 쿼리시 연산 속도가 느린 시각화 BI는 정제가 완료된 Gold 레이어 내 데이터를 사용하는 것이 좋습니다. 현재 Gold 레이어도 S3에 저장하지만 프로젝트별로 다른 경로로 저장됩니다. 태블로와 같이 쿼리시 연산 속도가 느린 시각화 BI는 정제가 완료된 Gold 레이어 내 데이터를 사용하는 것이 좋습니다. 데이터 파이프라인을 구성할 때 가장 중요한 것은 데이터 다운 타임을 최소화하는 것입니다. 데이터 다운 타임은 데이터의 오류로 인해 서비스의 가동이 중지되는 상황을 의미합니다. Appsflyer가 제공하는 Data Locker 데이터를 통해 고객의 유입을 확인할 수 있습니다. 그리고 세션 단위의 화면 노출 및 클릭 행동을 트래킹 하는 앰플리튜드 데이터가 존재합니다. 그리고 정산 및 정확한 비즈니스 로직을 위해 필요한 RDS 내 데이터가 있죠. 구체적인 파이프라인 고안 전에 데이터 소스의 적재 빈도, 데이터 스키마, 적재되는 과정을 살펴보았습니다. 여러 소스가 존재하고 각 소스의 적재 시간 단위가 동일하지 않은 점(실시간으로 쌓이는 데이터가 존재하고 시간 단위로 적재되는 데이터도 존재해요!)을 알 수 있었습니다. 그리고 비즈니스 로직상 적재량의 차이도 상대적으로 심한 것을 확인했습니다. 파이프라인을 운영하면서 다운 타임에 가장 큰 영향을 끼친 것은 Bronze layer에서 데이터가 적시에 적재가 되지 않은 신선도(Freshness)문제입니다. 신선도는 결국 데이터의 최신성을 파악하는 것이 중요합니다. 신선도와 관련된 파라미터를 조정하는 것은 비즈니스 맥락에 따라 달라집니다. 예를 들어 임프레션당 셀러에 과금을 계산하는 광고 로직에 필요한 테이블을 대상으로는 업데이트 최신성의 단위가 짧아야 합니다. 그리고 신선도 지표의 대상이 되는 테이블의 이상치 기준을 낮게 잡아 재현율을 높여서 실제 이상 징후를 최대한 많이 포착해야 합니다. 신선도와 함께 실제 데이터 파이프라인에서 많은 오류의 근본적인 원인에 스키마 변화와 데이터 분포 변화가 있었습니다. 위의 문제를 해결하기 위해 Schema Validation과 Data Drift를 잡는 알고리즘을 설계 하였습니다. 구체적으로 Schema Validation은 스파크를 통해 별도의 유효성 로직(i.e. Null값 사용 여부, 컬럼별 범위 제한, 고유 값 유무)을 적용하였습니다. Data Drift 같은 경우 기본적인 알고리즘(i.e. Moving Average, KL divergence)을 통해 자동으로 파악하여 대처할 수 있었습니다. 현재 팀 내 데이터 파이프라인 내 데이터 집계를 Athena(아테나)를 활용하는 부분이 많은데요. 아테나는 스캔한 데이터 양만큼 과금하므로 데이터를 분할 저장하여 원하는 파티션만큼 검색할 수 있는 파티셔닝이 중요합니다. 또한 데이터 파이프라인에서 지켜야 할 멱등성(특정 이유로 데이터 파이프라인을 여러번 실행해도 결과가 다르지 않게 하는 것!)과 원천 데이터의 완전성 원칙을 지키는 것이 중요합니다. 2. 데이터 파이프라인 외부의 외재적 상황 다루기 비즈니스 문제를 풀기 위한 데이터 파이프라인은 필연적으로 여러 이해 관계자와 연결됩니다. 회사의 비즈니스는 디스플레이 광고와 검색 결과 광고를 포함하는 리테일 미디어(Retail Media)를 포함하고 있습니다. 이와 관련하여 중요한 지표는 앱 내 고객의 활동성과 관련된 지표(i.e. 서비스 체류 시간)임을 데이터 분석을 통해 알 수 있습니다. 고객의 활동성 지표를 AARRR 프래임워크 중 Activation과 Retention으로 해석하고, 빠르게 대시보드에 표현하기 위해서는 앞에서 언급한 대로 패션 커머스 플랫폼은 크게 세 축으로 구성되어 있습니다. 세 축(고객, 상품, 셀러)으로 데이터가 Silver Data Layer에 적재되어야 합니다. 해당하는 주요 축은 도메인 및 풀고자 하는 비즈니스 문제에 따라 많이 다른데요. 중요한 점은 데이터 파이프라인 설계 시 개별적인 태스크 별로 공통적으로 사용할 수 있는 피처(feature)들을 발견하고 중복되는 연산량을 줄이는 것입니다. 이것을 효율적으로 해결하기 위해서 다양한 태스크에서 주요 축 단위로 정돈된 피처를 한 저장소에서 확인할 수 있어야 하는데, 해당 데이터 스키마를 저장하고 확인하는 작업은 글루 카탈로그와 Unity Catalog의 메타 스토어를 통해 원활히 진행할 수 있습니다. 팀 내부에서는 보다 빠른 대시보드 설계를 원했습니다. 그래서 데이터 아키텍처상 주요 저장소가 S3 기반인 AWS 내 브론즈 데이터와 실버 데이터를 Unity Catalog를 통해 데이터브릭스에 연결하였는데요. Unity Catalog와 연동된 Databricks SQL 서비스에 접근 가능했습니다. 작업 이후 자유롭게 정현님과 SQL 문법을 통해 주요 페이지 단위 대시보드를 제작할 수 있었습니다. 다만 전사 공유가 필요한 순간이 오면서 데이터브릭스 내 대시보드는 확장성이 떨어졌습니다. 왜냐하면 저희팀을 제외한 나머지 모든 팀은 태블로를 통해 데이터 및 대시보드 공유를 진행하기 때문입니다. 그래서 현재는 AWS 서비스와 태블로를 결합하여 의미 있게 발견한 데이터 인사이트를 대시보드로 제공하고 있습니다. 🏈 대시보드를 실제로 어떻게 구성했나요? 지금까지 대시보드 작성 전 지표의 선정 및 개발 과정을 소개하고, 그 이후 데이터 파이프라인을 통한 자동화 과정을 순철님이 설명드렸습니다. 이번에는 데이터와 지표가 준비된 상황에서 대시보드를 구성하고 지표를 시각화하는 과정을 말씀드리려 합니다. 좋은 시각화란? 이 단계에서 중요한건 “어떻게 데이터를 효과적으로 시각화할 것인가”의 문제입니다. 좋은 시각화란 무엇인지 몇가지 원칙을 설정하고 이에 맞게 각 차트를 구성하고자 하였습니다. 하나의 차트에서는 하나의 메시지를 전달해야 함 과한 색깔 사용 등으로 독자의 시선이 분산되지 않아야 함
색깔을 제한적이고 효과적으로 사용하기, 텍스트의 정렬을 통일하기 등 색깔을 제한적이고 효과적으로 사용하기, 텍스트의 정렬을 통일하기 등 사람이 옆에서 설명하지 않고 차트만 보았을 때에도 의도한 정보를 읽을 수 있어야 함 결국 비즈니스 의사결정에 직접적인 도움이 되는 인사이트를 제공해야 함 위의 사항을 고려한 몇가지 예시를 살펴보겠습니다. (1) 하나의 차트에서 하나의 정보만을 전달하기 위해 왼쪽과 같이 두 가지 내용이 한꺼번에 들어가있는 그래프를 오른쪽과 같이 두개의 그래프로 쪼개서 수정하였습니다. (2) 독자의 시선을 분산시키지 않기 위해 차트의 형식을 변형하였습니다. 아래와 같이 텍스트의 정렬이 바뀌며 시선을 분산하여 한 눈에 알아보기 어려운 왼쪽 그래프를 오른쪽의 방식으로 개선하였습니다. DATABRICKS SQL VS ATHENA + TABLEAU 대시보드는 두 차례에 걸쳐 각기 다른 툴을 사용하여 제작하였습니다. 첫 번째로는 팀 내부 공유 용으로 데이터브릭스를 활용했고 그 다음으로는 전사 공유를 위해 아테나와 태블로를 사용하여 제작했습니다. 데이터브릭스는 여러 데이터 소스를 연결하여 데이터를 관리할 수 있을 뿐 아니라 ML 파이프라인을 구성해 자동화를 진행하거나 BI 툴로써 시각화 및 대시보드 작성이 가능합니다. 한편, 아테나는 AWS의 S3에 적재된 데이터를 분석할 수 있는 쿼리 서비스이며 태블로는 데이터 시각화에 특화되어 있는 전문 툴입니다. (1) 데이터브릭스 데이터 브릭스 시각화의 큰 장점은 빠르고 간편하다는 것입니다. 데이터 브릭스 툴 안에서 SQL로 쿼리를 작성하면 한 창에서 바로 시각화가 가능하고, 해당 차트를 그대로 대시보드에 추가할 수 있습니다. 게다가 쿼리 결과가 적절한 형태로 주어졌을 때, 시각화 타입을 골라서 몇가지 값만 작성해주면 알아서 차트를 그려줍니다. 그러나 차트를 그리거나 대시보드를 구성하는 데에 있어서 자유도는 적은 편입니다. 예컨대 퍼널 차트를 그리고 싶다면 반드시 아래 예시와 같은 형태로만 그릴 수 있으며, 텍스트나 이미지 작성 역시 HTML 기본 문법을 통해서만 수정이 가능했습니다. 사실 기존 다른 팀은 모두 태블로로 데이터 및 대시보드를 공유해왔는데요. 데이터브릭스 대시보드로 바로 전사 공유가 가능할지 검토해보았지만 권한 문제가 있었습니다. 대시보드를 ‘볼 수 있는 사람’과 ‘데이터 테이블에 접근 가능하고 쿼리를 수정할 수 있는 사람’ 간의 권한 차등 부여가 까다로웠습니다. 대시보드를 보려면 반드시 새로고침이 필요했는데, 이를 위해서 쿼리 및 테이블에 대한 접근 권한 모두가 필요했기 때문이죠. 결국 확장성과 권한 부여 문제로 인해 전사 공유를 위한 대시보드는 태블로로 작성하기로 결정하였습니다. (2) 아테나 + 태블로 대시보드를 제작하기 위해 사용한 원본 데이터 소스는 S3에 적재되어 있습니다. 이 테이블을 곧바로 태블로에 연결하여 태블로 내에서 수억건의 row를 읽어와 쿼리로 작업을 하게 될 경우 속도가 매우 느려지는 문제가 있습니다. 그래서 Silver Layer내 다른 태스크를 위해 이미 피처가 생성되는 경우 중복으로 계산하지 않았습니다. 그리고 중복 데이터를 활용하여 쿼리를 두번 이상 사용하는 경우 미리 데이터를 Silver Layer에 정제하여 적재하였습니다. 아테나에서 쿼리 작업을 하는 것은 데이터브릭스 SQL의 쿼리 작업과 동일했어요. 다만 둘은 지원하는 문법이 달라 약간의 수정을 거쳐야 했습니다. 아테나는 프레스토 DB 기반 표준 SQL 문을 지원하는 반면, 데이터브릭스는 Databricks SQL이라는 Spark SQL 기반 변형 문법을 사용합니다. 그래서 아래와 같이 쿼리 간의 차이가 존재합니다. 날짜 계산 쿼리 차이 따옴표 쿼리 차이 나눗셈 연산 쿼리 차이 적재한 테이블을 기반으로 태블로에서 시각화를 했습니다. 태블로는 데이터브릭스의 시각화보다 훨씬 많고 다양한 기능을 제공합니다. 정확히는 자유도가 높습니다. 예컨대 똑같은 퍼널을 그릴 때에도 데이터브릭스는 정해진 형식으로 차트를 그려주는 반면 태블로는 보여줄 내용과 방식, 디자인 등 모든 것을 사용자가 지정할 수 있습니다. 데이터브릭스보다 사용법을 익히기 훨씬 복잡하지만, 그만큼 더 정교한 시각화가 가능한 셈입니다. 🏀 그래서 어떻게 돈이 됐는데요? 대시보드 제작 이후 지속적으로 지표를 트래킹하며 몇 가지 크고 작은 이슈를 발견하게 되었어요. 그리고 이 문제를 해결함으로써 추가적인 수익을 낼 수 있었습니다. (1) 상품 노출 임프레션 누수 발견 후 해결 아래와 같이 예시페이지를 보면, 한 페이지 안에 여러 조닝이 보이는 것을 볼 수 있습니다. 그런데 이 조닝별 노출 유저 수를 퍼널 차트로 살펴보니, “A영역의 상품 노출 임프레션에 분명 노출이 되었는데도 노출 유저로 잡히지 않는 임프레션 누수가 생기고 있음”을 확인할 수 있었습니다. 해당 누수 발견 후 AOS / IOS의 UI를 변경함으로써 해당 문제를 해결하였고 그 과정에서 잡을 수 있게 된 임프레션 수 만큼의 실제 금전적인 효과를 얻게되었습니다. 아래 차트를 보시면 그 전에 비해 노출이 잘 잡히고 있네요. (2) 검색 및 추천 최적화 반영 검색 대시보드의 경우, 검색 팀내 피드백을 반영하여 검색 주요 지표 모니터링을 진행하고 있습니다. 그래서 슬롯 정책 변경으로 2024년 1월 말 대비 2주 평균 매출 21% 매출 상승, 3월에는 40%까지 매출 증가가 예상됩니다. 또한 CTR 및 검색 관련 지표의 추이를 확인한 결과 슬롯 정책 변화와 고객의 피드백 데이터 사이에 큰 차이가 없었습니다. 추가적으로 페이지 간의 관계성과 조닝 내 상품 진열에 대한 고객의 반응을 종합적으로 파악할 수 있었습니다. 트래픽을 많이 발생시키는 주요 페이지의 특징을 통해 검색 최적화 및 개인화에 대한 방향성을 잡을 수 있습니다. (3) 새로운 앱 내 기능 도입 예정 앞에서 말씀드린 것처럼 회사의 비즈니스는 디스플레이 광고와 검색 결과 광고를 포함하는 리테일 미디어(Retail Media)를 포함하고 있습니다. 체류시간이 지속적으로 감소하는 현상 발견 이후, 이를 기반으로 새로운 앱 내 기능을 도입할 예정입니다. (4) 실험 비즈니스 로직 관련 이슈 발견 후 해결 “글의 초입에서 말씀드렸던 실험 결과, 비즈니스 로직을 제거하지 않기로 결정”했다고 말씀드린 바 있는데요. 실험 기간이 끝났는데도 실험 관련 로직이 포함된 채로 상품이 송출되고 있음을 발견했습니다. 이 문제를 대시보드를 통해 발견하고 즉시 수정함으로써 이슈를 빠르게 발견하고 해결할 수 있었습니다. 🎾 개선할 점은 무엇인가요? 지금까지 지표를 개발하고 데이터 파이프라인을 구성하고 최종적으로 대시보드를 만들어 수익을 창출한 과정을 말씀드렸습니다. 여기에는 몇 가지 남아있는 과제가 있습니다. (1) 유저 세그멘테이션 모델이 반영된 세부 분석 현재 저희팀은 CRM 최적화 태스크를 위해 ML 모델을 도입 중에 있는데요. 구체적으로 구매 확률 예측 모델, RFM 그리고 상품 소재에 반응하는 유저 세그멘테이션 모델을 도입 중입니다. 대시보드에서도 도입된 ML 모델과 유저의 피드백 데이터를 활용해 세그멘테이션 별 세부적인 코호트 분석이 필요합니다. 현재 대시보드는 현황을 파악하고 문제를 발견하는 데에 초점이 맞춰져 있습니다. 위와 같이 다양한 코호트별 퍼널 분석 등 세분화된 분석이 추가된 이후에는 문제의 원인까지 파악할 수 있는 대시보드가 될 것입니다. (2) 주요 페이지 및 조닝의 특성에 맞는 추천 최적화 및 세부 분석 주요 페이지를 분석하면서 같은 고객도 세션 및 맥락에 따라 다른 행동을 하는 것을 발견할 수 있습니다. 그래서 추천 모델 및 비즈니스 로직이 결합한 상품 진열과 고객의 피드백 데이터 사이에 어떤 관련성이 존재하는지를 지표를 통해 확인해야 합니다. 발견되는 지표를 통해 주요 조닝의 추천 모델을 최적화하여 고객에게 보다 나은 상품을 먼저 제공할 수 있습니다. (3) Data Quality System 현재는 발생 가능한 정형 데이터와 비정형 데이터가 Data Lake에 모두 적재되는 구조입니다. 데이터 품질을 항상 체크하고 데이터 파이프라인 관련 이슈 발생시 빠르게 오류의 원인을 파악하는 것이 중요한데요. 현재는 운영을 하면서 빈번하게 다운 타임이 증가한 영역에 대한 부분적 데이터 검증(Data Validation)을 진행하였습니다. 이 부분을 확장하여 데이터 적재부터 고객의 피드백 과정까지 발생할 수 있는 미묘한 데이터 오류 부분을 빠르게 파악하고 대응할 수 있는 시스템이 필요합니다. (4) 저희가 운영하고 있는 다른 서비스로 확장 현재 지표 기획, 데이터 파이프라인, 그리고 자동화 대시보드 제작은 브랜디 서비스를 대상으로 진행했습니다. 추후 운영하고 있는 다른 서비스로 확장이 필요합니다. PS.지표를 기획하고 대시보드를 자동화하는 과정에서 도움 주시고 조언해 주신 AI검색팀과 데이터 개발팀에 감사드립니다. 그리고 플랫폼 관점에서 중요한 피드백을 주신 TECH 플랫폼 조직 상근님께 감사드리며 글을 맺습니다. Reference 이커머스 데이터 분석과 개선을 위한 작은 실험(팀 내부 프로젝트) RFM 분석 적용 내용(팀 내부 프로젝트) CRM 최적화 프로젝트 - 쿠폰 발급 최적화(팀 내부 프로젝트) How to do data quality with DataOps How we deal with Data Quality using Circuit Breakers The New Rules of Data Quality A Deep Dive Into Data Quality 3 Steps to Improve the Data Quality of a Data lake How to monitor Data Lake health status at scale Automated Data Quality Testing at Scale using Apache Spark Testing data quality at scale with PyDeequ Implementing Data Quality with Amazon Deequ & Apache Spark python-deequ github Time Series Aggregations with Core PySpark Data Quality — You’re Measuring It Wrong Data Quality at Airbnb Analytics ICON Funnel Chart - Suggested Alternatives 7 Data Storytelling Techniques to Build Dashboards That Engage your Customers AARRR 의미 자세히 파헤쳐보기(퍼널 분석 이미지 출처) 브랜디 랩스(Brandi Labs) Site Map Posting Recruiting Contact Contact 070-7605-0704 recruit@brandi.co.kr facebook instagram App download Android iOS","지난해 말부터 커머스 유저 데이터를 분석 및 지표를 기획하고 대시보드로 자동화 하는 업무를 진행하였고 이를 통해 추가적인 수익을 얻게 되었고 데이터를 통한 최적화 방향을 잡을 수 있었다. 극적으로는 사용자가 해당 서비스를 더 적극적으로 이용하고 더 많은 수익을 창출하는 건강한 사용자 여정을 만들기 위해 대시보드 작업을 시작했는데 데이터 분석가의 관점을 반영해 필요한 지표를 통합하여 서비스의 현재 상태를 통합적으로 볼 필요가 있었기에 대시보드의 관점을 다음과 같다. AARRRR 중에서도 가장 중요한 것은 Activation과 Retention으로 고객의 활동성을 정의할 수 있는 방식이 다양하기 때문에 고객의 활동성에 관한 주요 질문을 던지고 질문의 답이 될 수 있는 지표를 제시하는 방식으로 개발하였다. 현재 저희팀은 검색, 추천 그리고 CRM 최적화 작업을 진행하면서 비즈니스와 밀접한 광고실 및 유관 부서에 데이터 분석 리포팅을 제공하고 있으며 데이터 파이프라인을 설계할 때도 도메인에 대한 이해가 필요하다. 비즈니스 문제를 풀기 위한 데이터 파이프라인 데이터 파이프라인은 결국 조직이 마주친 비즈니스 문제를 풀고 현상을 파악하기 위해서 존재한다. 데이터 적재시 신선도 측면에서 가장 다운 타임이 증가하는 데이터 레이어인 Bronze data layer는 데이터 분석가, 데이터 사이언티스트, ML 엔지니어가 주로 사용하는 데이터 레이어다. Bronze layer에서 데이터가 적시에 적재가 되지 않은 신선도(Freshness)문제를 해결하기 위해 Schema Validation과 Data Drift를 잡는 알고리즘을 설계 하였다. 현재 팀 내 데이터 파이프라인 내 데이터 집계를 Athena(아테나)를 활용하는 부분이 많은데 데이터 파이프라인에서 지켜야 할 멱등성(특정 이유로 데이터 파이프라인을 여러번 실행해도 결과가 다르지 않게 하는 것)과 원천 데이터의 완전성 원칙을 지키는 것이 중요하다. 데이터 아키텍처상 주요 저장소가 S3 기반인 AWS 내 브론즈 데이터와 실버 데이터를 Unity Catalog를 통해 데이터브릭스에 연결하여 의미 있게 발견한 데이터 인사이트를 대시보드로 제공하고 있다. 데이터 브릭스는 여러 데이터 소스를 연결하여 데이터를 관리할 수 있을 뿐 아니라 ML 파이프라인을 구성해 자동화를 진행하거나  툴로써 시각화 및 대시보드 작성이 가능하며 데이터 브릭스 툴 안에서 SQL로 쿼리를 작성하면 한 창에서 바로 시각화가 가능하고, 해당 차트를 그대로 대시보드에 추가할 수 있다. 확장성과 권한 부여 문제로 전사 공유를 위한 대시보드는 태블로로 작성하기로 결정한 아테나는 데이터브릭스 SQL의 쿼리 작업과 동일했으나, 데이터브릭스의 시각화보다 훨씬 많고 다양한 기능을 제공한다. 조닝별 노출 유저 수를 퍼널 차트로 살펴보니 “A영역의 상품 노출 임프레션에 분명 노출이 되었는데도 노출 유저로 잡히지 않는 임프레션 누수가 생기고 있음”을 확인할 수 있어 해당 누수 발견 후 해당 문제를 해결하였고 그 과정에서 잡을 수 있게 된 임프레션 수 만큼의 실제 금전적인 효과를 얻게되었다. 현재 대시보드는 현황을 파악하고 문제를 발견하는 데에 초점이 맞춰져 있지만 현재는 데이터 품질을 항상 체크하고 데이터 파이프라인 관련 이슈 발생시 빠르게 오류의 원인을 파악하는 것이 중요하다. PS.지표를 기획하고 대시보드를 자동화하는 과정에서 도움 주시고 조언해 주신 AI검색팀과 데이터 개발팀에 감사드립니다."
3,3,"Python으로 이해하는 주가지표① - RSI(Relative Strength Index)
","https://surfit.io/link/zpebN
","글쓰기 방명록 RSS 관리   Python으로 이해하는 주가지표① - RSI(Relative Strength Index) (feat. 엔비디아)   RSI란 무엇일까? RSI는 가격의 상승압력과 하락압력 간의 상대적인 강도를 나타낸다고 한다. (출처) (무슨 말인지 직관적으로 이해가 잘 안되지만.. 일단 pass)   RSI는 그 종목이 과매수 상태인지 과매도 상태인지를 판단할 때 사용한다. 일반적으로 RSI가 70% 이상이면 과매수, 30% 이하면 과매도로 판단한다.   RSI는 다음과 같은 복잡한 식으로 표현된다.   위 식을 파이썬으로 나타내보고 지표를 직관적으로 이해해보고, 엔비디아 주가에서 RSI를 뽑아보자! RSI 뽑아보기 RSI를 뽑기 위한 랜덤 데이터를 만들어보자.  다음은 2024년 1월 1일부터 20일 간 랜덤으로 주가 데이터를 만드는 코드이다.   만들어진 데이터로 이전 날짜 대비 증감을 계산해보도록 하자.   Price Change라는 값은 날짜 증감을 담고 있다.   다음은 이득과 손실을 계산해보자. 증감이 0보다 큰지 여부에 따라 값을 그대로 가져오면 된다.   다음으로, 평균 이득과 평균 손실을 계산해야하는데, 일반적으로 14일을 기준으로 평균을 계산한다고 한다.   14일 평균으로 계산했기 때문에 13행(index = 12)까지는 값이 없다.   이번엔 RS를 뽑아보자! 	RS는 1을 기점으로 분포되어있다. 평균 이득이 더 크다면 1보다 큰 수를 가지고 평균 손실이 더 크다면 1보다 작은 수를 가진다.   따라서, RS가 큰 값을 가질수록 최근 14일간 손실보다 이득이 더 크다는 것을 의미하고 RS가 작은 값을 가질수록 최근 14일간 이득보다 손실이 더 크다는 것을 의미한다!   최종적으로 우리의 목표인 RSI를 구해보자! 	RSI가 30 ~ 70 사이에 존재하므로, 과매도 혹은 과매수 구간은 아닌 것으로 보인다.   그럼 RSI의 의미를 살펴볼까? 최근 14일의 평균 이득과 손실이 같다면, RS = 1이다. RS = 1이라면 RSI는 50이 된다. 즉, RSI가 50이라면 평균 이득과 평균 손실이 동일한 상태임을 의미한다!   자 이번엔 RSI의 기준이었던 30과 70의 의미를 역산을 통해 알아보면 RSI = 30이라면, RS = 3 / 7이어야한다.
따라서, 평균 이득 : 평균 손실 = 3 : 7이어야 하며,
이는 평균 이득보다 평균 손실이 2.3배 이상일 때를 의미한다! 따라서, 평균 이득 : 평균 손실 = 3 : 7이어야 하며, 이는 평균 이득보다 평균 손실이 2.3배 이상일 때를 의미한다! RSI = 70이라면, RS = 7 / 3이어야 한다.
따라서, 평균 이득 : 평균 손실 = 7 : 3이어야 하며,
이는 평균 손실보다 평균 이득이 2.3배 이상일 때를 의미한다! 따라서, 평균 이득 : 평균 손실 = 7 : 3이어야 하며, 이는 평균 손실보다 평균 이득이 2.3배 이상일 때를 의미한다! 엔비디아 주가에서 RSI 뽑아보기 요즘 엔비디아 주가는 연일 최고치를 경신하고 있다. (곧 조정이 오겠지... 라며 구경만 하다가 못들어간 사람이 바로 나다🥲)   yfinance를 통해 엔비디아 주가를 불러와서 RSI를 확인해보자! (yfinance가 무엇인지 모르겠다면? → 링크 클릭해보기!)       RSI에 따르면, 1월 11일에 80을 기록하며 과매수 구간에 진입 2월 20일에 65를 기록하며 과매수 구간 탈출 그 이후 70 선을 횡보 함을 알 수 있다! 요약 RSI를 통해 현재 시점이 과매수 혹은 과매도 구간인지 판단할 수 있다. RSI는 최근 n일 간 평균 이득과 평균 손실의 비율을 나타내며 2.3배를 기준으로 과매도, 과매수를 판단한다. 엔비디아는 광기다 🔄데이터 분석가가 써먹는 전환 분석의 다섯 가지 접근법(feat. 로그 데이터, SQL)
2024.10.27 GPT한테 SQL 쿼리 짜달라고 하기
2024.03.17 yfinance(야후 파이낸스)로 주가 데이터 손질하기
2024.01.07 Retentioneering: 그 복잡한 product data를 간단하고 자유도 높게 다룰 수 있다고?
2023.12.17",RSI는 그 종목이 과매수 상태인지 과매도 상태인지를 판단할 때 사용하는데 RS가 큰 값을 가질수록 최근 14일간 이득보다 이득이 더 크다는 것을 의미하고 RS가 작은 값을 가질수록 최근 14일간 이득보다 손실이 더 크다는 것을 의미한다.
